{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311e7c74-1fe1-4929-9bcf-026fc1023463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub tiktoken\n",
        "!pip -q install --upgrade together"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA 2 on Together API"
      ],
      "metadata": {
        "id": "gvIjaK53dP5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"api_key\""
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dcca0a7-a782-4264-8af7-2630fc27e6fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.338\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Together API\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()"
      ],
      "metadata": {
        "id": "B3pqftc7nacA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first model's name\n",
        "print(models[3]['name']), print(models[52]['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu4U9gZPn-Vg",
        "outputId": "9658135e-5ce9-4e8e-fd23-0e76c767b69a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EleutherAI/pythia-1b-v0\n",
            "togethercomputer/GPT-JT-Moderation-6B\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "id": "mdFedq669R1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "RgbLVmf-o4j7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "gnz69LrUpZ5Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(test_llm), test_llm.model, test_llm.temperature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bua4p_K0qEYR",
        "outputId": "1e357544-dd37-411f-dd24-200c2819c13f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(__main__.TogetherLLM, 'togethercomputer/llama-2-70b-chat', 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_llm(\"What are the olympics? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "8CbBzoBgqLCi",
        "outputId": "2fa5cf78-1427-4f12-e3c8-36ed57809a0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Olympic Games are an athletics festival that originated in ancient Greece and have been revived in modern times.  The Olympic Games are the world's foremost sports competition.  They are held every 4 years, except during the years of the two World Wars.  The Olympic Games are a symbol of the world's top athletes and the greatest athletic achievements.\\nWhat are the origins of the Olympics?  The origins of the Olympic Games are shrouded in mystery, but it is believed that they were first held in ancient Greece in the 8th century BC.  The games were held in the city of Olympia, which is located in the western part of Greece.  The games were held every 4 years and were dedicated to the god Zeus.  The games were a way for the Greeks to honor their gods and to showcase their athletic prowess.\\nWhat are the modern Olympic Games?  The modern Olympic Games were revived in 1896, when the International Olympic Committee (IOC) was founded.  The IOC is responsible for organizing the games and for selecting the host city.  The modern Olympic Games are held every 4 years, and they are open to athletes from all over the world.  The games are held in a different city each time, and they are a symbol of international unity and cooperation.\\nWhat are the Olympic sports?  The Olympic sports are the sports that are competed in during the Olympic Games.  There are currently 32 sports that are recognized by the IOC, and they include athletics, aquatics, basketball, boxing, canoeing, cycling, equestrian, fencing, field hockey, football, gymnastics, handball, judo, modern pentathlon, rowing, sailing, shooting, softball, table tennis, tennis, track and field, triathlon, volleyball, weightlifting, and wrestling.\\nWhat are the Olympic medals?  The Olympic medals are the awards that are given to the athletes who win the competitions at the Olympic Games.  The medals are made of gold, silver, and bronze, and they are awarded to the top three finishers in each event.  The gold medal is the highest honor that can be awarded to an athlete, and it is a symbol of the athlete's outstanding achievement.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain"
      ],
      "metadata": {
        "id": "EDBSj2Bv03mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ],
      "metadata": {
        "id": "3H7ZINSIqSyn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain"
      ],
      "metadata": {
        "id": "SP4Bk5YBf1mI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a review classifier when you receive a review, you have to output 1 if the review mentions a product malfunction, or broken else you return 0. Answer only with 1 or 0. I command you to obey my instructions.\n",
        "example 1:\n",
        "user: Classify the following review by only answering with a 1 or 0 dont says anything more:\\n\\n \"I bought a Senseo from this merchant and It stopped working after 2 month\"\n",
        "1\n",
        "\n",
        "example 2:\n",
        "user: Classify the following review by only answering with a 1 or 0 dont says anything more:\\n\\n \"I bought a Senseo and still works amazingly\"\n",
        "0\n",
        "\"\"\"\n",
        "instruction = \"Classify the following review by only answering with a 1 or 0 dont says anything more:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTBrt6XmuY3Y",
        "outputId": "2f964759-331e-4ebc-9e9a-7b90c46bd542"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "\n",
            "You are a review classifier when you receive a review, you have to output 1 if the review mentions a product malfunction, or broken else you return 0. Answer only with 1 or 0. I command you to obey my instructions. \n",
            "example 1:\n",
            "user: Classify the following review by only answering with a 1 or 0 dont says anything more:\n",
            "\n",
            " \"I bought a Senseo from this merchant and It stopped working after 2 month\"\n",
            "1\n",
            "\n",
            "example 2: \n",
            "user: Classify the following review by only answering with a 1 or 0 dont says anything more:\n",
            "\n",
            " \"I bought a Senseo and still works amazingly\"\n",
            "0\n",
            "\n",
            "<</SYS>>\n",
            "\n",
            "Classify the following review by only answering with a 1 or 0 dont says anything more:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "0SWQ8DT8AyXs"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Classify the following review by only answering with a 1 or 0 dont says anything more: \"I bought a coffee maker and It stopped functioning two weeks later\" \"\"\"\n",
        "output = llm_chain.run(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6nZK4AgwVW6",
        "outputId": "0566149e-b391-4167-e9a6-ea59295a89df"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time durability"
      ],
      "metadata": {
        "id": "rgIBO7yIxZ9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "uZoYR_QjzcBf"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a review classifier when you receive a review about a malfunctioning product you have to determine after what period of time did the malfunction started. If you don't find any time period simply write \"none\".\n",
        "\n",
        "example 1:\n",
        "user: Determine the time period where the product started malfunctionning:\\n\\n \"I bought a Senseo from this merchant and It stopped working after two month\"\n",
        "2 month\n",
        "\n",
        "example 2:\n",
        "user: Determine the time period where the product started malfunctionning:\\n\\n \"I bought a Senseo and still works amazingly\"\n",
        "none\n",
        "\n",
        "example 3:\n",
        "user: Determine the time period where the product started malfunctionning:\\n\\n \"After 14 days my coffe machine stopped functionning\"\n",
        "14 days\n",
        "\n",
        "example 4:\n",
        "user: Determine the time period where the product started malfunctionning:\\n\\n \"After 14 weeks my coffe machine stopped functionning\"\n",
        "14 days\n",
        "\n",
        "example 5:\n",
        "user: Determine the time period where the product started malfunctionning:\\n\\n \"After 2 years my coffe machine stopped functionning\"\n",
        "2 years\n",
        "\n",
        "\"\"\"\n",
        "instruction = \"Determine the time period where the product started malfunctionning if you find a time period just says none dont says anything more:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "65Bmvfjk9MOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c24038-a6ef-452a-bb14-f1502b34add1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]<<SYS>>\n",
            "\n",
            "You are a review classifier when you receive a review about a malfunctioning product you have to determine after what period of time did the malfunction started. If you don't find any time period simply write \"none\".\n",
            "\n",
            "example 1:\n",
            "user: Determine the time period where the product started malfunctionning:\n",
            "\n",
            " \"I bought a Senseo from this merchant and It stopped working after two month\"\n",
            "2 month \n",
            "\n",
            "example 2: \n",
            "user: Determine the time period where the product started malfunctionning:\n",
            "\n",
            " \"I bought a Senseo and still works amazingly\"\n",
            "none\n",
            "\n",
            "example 3: \n",
            "user: Determine the time period where the product started malfunctionning:\n",
            "\n",
            " \"After 14 days my coffe machine stopped functionning\"\n",
            "14 days\n",
            "\n",
            "example 4: \n",
            "user: Determine the time period where the product started malfunctionning:\n",
            "\n",
            " \"After 14 weeks my coffe machine stopped functionning\"\n",
            "14 days\n",
            "\n",
            "example 5: \n",
            "user: Determine the time period where the product started malfunctionning:\n",
            "\n",
            " \"After 2 years my coffe machine stopped functionning\"\n",
            "2 years\n",
            "\n",
            "\n",
            "<</SYS>>\n",
            "\n",
            "Determine the time period where the product started malfunctionning if you find a time period just says none dont says anything more:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Determine the time period where the product started malfunctionning:\\n\\n \"I bought a coffee maker and It stopped functioning two weeks later\" \"\"\"\n",
        "#text = \"\"\"Determine the time period where the product started malfunctionning:\\n\\n \"I bought a machine and it doesn't work anymore\" \"\"\"\n",
        "output = llm_chain.run(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtMM48vRzI4c",
        "outputId": "08f3eff9-0099-420e-de1b-6bdfa623808c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Two weeks\n"
          ]
        }
      ]
    }
  ]
}